import json
import time
import random
import re
from selenium import webdriver
from selenium.webdriver.chrome.options import Options
from bs4 import BeautifulSoup
from urllib.parse import urljoin

# ä¸‹è¼‰å‹Ÿè³‡å¹³å°è³‡æ–™(æŠ“ ç›®æ¨™ / éå»é›†è³‡   è´ŠåŠ©äººæ•¸  å‰©é¤˜å¤©æ•¸  å°ˆæ¡ˆæœŸé–“èµ·è¨–)
# (ç›¡é‡åœ¨å‡½æ•¸å…§ä¾‹å¤–è™•ç†)

DATA_PATH      = 'data.json'
SLEEP_ON_ERROR = 180  # æ’éŒ¯å¾Œç­‰å€™ç§’æ•¸
SLEEP_BETWEEN  = 60   # æ¯ç­†è™•ç†å®Œç­‰å€™ç§’æ•¸

def load_projects(path):
    """è®€å– JSON æª”æ¡ˆå›å‚³ list of dict"""
    try:
        with open(path, 'r', encoding='utf-8') as f:
            projects = json.load(f)
        print(f"âœ… æˆåŠŸè®€å– {path} ({len(projects)} ç­†å°ˆæ¡ˆ)")
        return projects
    except FileNotFoundError:
        print(f"âš ï¸ æ‰¾ä¸åˆ° {path}ï¼Œå›å‚³ç©ºåˆ—è¡¨")
        return []
    except Exception as e:
        print(f"âŒ è®€å– {path} å¤±æ•—ï¼š{e!r}")
        raise

def collect_link_items(projects):
    """å¾ projects è£¡æ‰¾å‡ºæ‰€æœ‰æœ‰ link çš„é …ç›®ï¼Œå›å‚³ list of (index, url)"""
    items = []
    for idx, proj in enumerate(projects):
        url = proj.get('link')
        if url:
            items.append((idx, url))
    print(f"ğŸ”— å…±æ‰¾åˆ° {len(items)} å€‹å« link çš„å°ˆæ¡ˆ")
    return items

def init_driver():
    """å•Ÿå‹•ä¸€å€‹éš±èº«ä¸”å½è£éçš„ Chrome driver"""
    opts = Options()
    opts.add_argument('--headless=new')
    opts.add_argument('--disable-blink-features=AutomationControlled')
    opts.add_argument('--incognito')
    opts.add_argument(
        'user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) '
        'AppleWebKit/537.36 (KHTML, like Gecko) '
        'Chrome/94.0.4606.61 Safari/537.36'
    )
    try:
        driver = webdriver.Chrome(options=opts)
        print("  âœ“ driver å•Ÿå‹•æˆåŠŸ")
        return driver
    except Exception as e:
        print(f"  âŒ driver å•Ÿå‹•å¤±æ•—ï¼š{e!r}")
        raise

def scrape_project(url, index):
    """
    è² è²¬ï¼š
      1. driver.get()
      2. æª¢æŸ¥ CAPTCHA
      3. è§£æä¸¦å›å‚³çµæœ dict
    """
    driver = init_driver()
    try:
        driver.get(url)
        time.sleep(random.uniform(1, 2))
        src = driver.page_source
        
        
           # é©—è­‰å­—çœ¼æª¢æŸ¥
        if "é©—è­‰" in src or "CAPTCHA" in src.upper():
            print("    âš ï¸ åµæ¸¬åˆ°é©—è­‰é é¢å­—çœ¼ï¼Œå¯èƒ½è¢«å°é–ï¼")
            path = f"debug_block_{index}.png"
            driver.save_screenshot(path)
            print(f"    å·²å­˜æˆªåœ–: {path}")


        # CAPTCHA åµæ¸¬
        #if 'é©—è­‰' in src or 'CAPTCHA' in src.upper():
         #   print("    âš ï¸ åµæ¸¬åˆ°é©—è­‰é é¢ï¼Œæˆªåœ– & ç­‰å¾…é‡è©¦")
          #  path = f"debug_block_{index}.png"
           # driver.save_screenshot(path)
           # time.sleep(SLEEP_ON_ERROR)
           # raise RuntimeError("CAPTCHA block")

        soup = BeautifulSoup(src, 'lxml')
        result = {}

        # ç›®æ¨™ / éå»é›†è³‡é‡‘é¡
        for tag in soup.find_all(['span', 'a'], class_='text-gray-500'):
            txt = tag.get_text(strip=True)
            if txt.startswith('ç›®æ¨™') or txt.startswith('éå»'):
                m = re.search(r'NT\$ *([\d,]+)', txt)
                if m:
                    key = 'TargetPrice' if txt.startswith('ç›®æ¨™') else 'PastPrice'
                    result[key] = int(m.group(1).replace(',', ''))

        # è´ŠåŠ©äººæ•¸
        b = soup.select_one('span.js-backers-count')
        if b:
            result['Backers'] = int(b.get_text(strip=True).replace(',', ''))

        # å‰©é¤˜å¤©æ•¸
        t_node = soup.select_one('h3.js-time-left.text-zec-green')
        if t_node:
            m = re.search(r'(\d+)', t_node.get_text())
            if m:
                result['TimeLeftDays'] = int(m.group(1))

        # å°ˆæ¡ˆæœŸé–“
        dur_txt = soup.select_one('h3.inline-block.text-gray-500.text-xs')
        if dur_txt:
            regex = (
                r"(\d{4}/\d{2}/\d{2}\s\d{2}:\d{2})"
                r"(?:\sâ€“\s(\d{4}/\d{2}/\d{2}\s\d{2}:\d{2}))?"
            )
            mt = re.search(regex, dur_txt.get_text())
            if mt:
                result['Duration'] = {
                    'begin': mt.group(1),
                    'end': mt.group(2) or ''
                }

        return result

    except Exception:
        # ç„¡æ³•è‡ªè¡Œè™•ç†çš„éŒ¯èª¤å¾€ä¸Šä¸Ÿ
        raise

    finally:
        driver.quit()
        print("  driver.quit() å®Œæˆ")

def save_projects(path, projects):
    """æŠŠ list of dict å¯«å› JSON æª”æ¡ˆ"""
    try:
        with open(path, 'w', encoding='utf-8') as f:
            json.dump(projects, f, ensure_ascii=False, indent=2)
        print(f"âœ… å·²æˆåŠŸå°‡è³‡æ–™å¯«å…¥ {path}")
    except Exception as e:
        print(f"âŒ å¯«å…¥ {path} å¤±æ•—ï¼š{e!r}")

def main():
    projects   = load_projects(DATA_PATH)
    link_items = collect_link_items(projects)

    for i, (orig_idx, url) in enumerate(link_items, start=1):
        
        if i==5:
            print("ğŸ”§ åœåœ¨ç¬¬ 5 ç­†åšæ¸¬è©¦")
            break
        
        print(f"\n[{i}/{len(link_items)}] è™•ç†ï¼š{url}")
        try:
            result = scrape_project(url, i)
        except Exception as e:
            print(f"  â†³ æ­¤ç­†å¤±æ•—ï¼š{e}ï¼Œè·³é")
            continue

        projects[orig_idx].update(result)
        print("    â†³ æˆæœï¼š", result)

        time.sleep(SLEEP_BETWEEN)

    save_projects(DATA_PATH, projects)

if __name__ == '__main__':
    try:
        main()
    except Exception as e:
        print(f"[FATAL] ç¨‹å¼ç•°å¸¸çµæŸï¼š{e}")