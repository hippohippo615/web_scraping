import json
import time
import random
import re
from selenium import webdriver
from selenium.webdriver.chrome.options import Options
from bs4 import BeautifulSoup

DATA_PATH = 'data.json'
SLEEP_ON_ERROR = 180  # æ’éŒ¯å¾Œç­‰å€™ç§’æ•¸
SLEEP_BETWEEN = 60    # æ¯ç­†è™•ç†å®Œç­‰å€™ç§’æ•¸

# ä¸‹è¼‰å‹Ÿè³‡å¹³å°è³‡æ–™(æŠ“ ç›®æ¨™ / éå»é›†è³‡   è´ŠåŠ©äººæ•¸  å‰©é¤˜å¤©æ•¸  å°ˆæ¡ˆæœŸé–“èµ·è¨–)
# æŠŠæ¯å€‹åŠŸèƒ½åŒ…è£æˆå‡½æ•¸


def load_projects(path):
    """è®€å– JSON æª”æ¡ˆå›å‚³ list of dict"""
    try:
        with open(path, 'r', encoding='utf-8') as f:
            projects = json.load(f)
        print(f"âœ… æˆåŠŸè®€å– {path} ({len(projects)} ç­†å°ˆæ¡ˆ)")
        return projects
    except Exception as e:
        print(f"âŒ è®€å– {path} å¤±æ•—ï¼š{e!r}")
        raise
        
        
        
def collect_link_items(projects):
    """å¾ projects è£¡æ‰¾å‡ºæ‰€æœ‰æœ‰ link çš„é …ç›®ï¼Œå›å‚³ list of (index, url)"""
    items = []
    for idx, proj in enumerate(projects):
        url = proj.get('link')
        if url:
            items.append((idx, url))
    print(f"ğŸ”— å…±æ‰¾åˆ° {len(items)} å€‹å« link çš„å°ˆæ¡ˆ")
    return items        



def init_driver():
    """å•Ÿå‹•ä¸€å€‹éš±èº«ä¸”å½è£éçš„ Chrome driver"""
    opts = Options()
    opts.add_argument('--headless=new')
    opts.add_argument('--disable-blink-features=AutomationControlled')
    opts.add_argument(
        'user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) '
        'AppleWebKit/537.36 (KHTML, like Gecko) '
        'Chrome/94.0.4606.61 Safari/537.36'
    )
    return webdriver.Chrome(options=opts)

def scrape_project(driver, url, index):
    """
    è² è²¬ï¼š
      1. driver.get()
      2. æª¢æŸ¥ CAPTCHA é—œéµå­—
      3. BeautifulSoup è§£æ
      4. å›å‚³ä¸€å€‹ dictï¼Œå…§å« TargetPrice, PastPrice, Backers, TimeLeftDays, Duration
    """
    result = {}
    print("  âœ“ driver.get() æˆåŠŸ")
    src = driver.page_source
    # é©—è­‰å­—çœ¼æª¢æŸ¥
    if "é©—è­‰" in src or "CAPTCHA" in src.upper():
        print("    âš ï¸ åµæ¸¬åˆ°é©—è­‰é é¢å­—çœ¼ï¼Œå¯èƒ½è¢«å°é–ï¼")
        path = f"debug_block_{index}.png"
        driver.save_screenshot(path)
        print(f"    å·²å­˜æˆªåœ–: {path}")

    soup = BeautifulSoup(src, 'lxml')

    # 3.5 ç›®æ¨™ / éå»é›†è³‡
    for tag in soup.find_all(['span', 'a'], class_='text-gray-500'):
        txt = tag.get_text(strip=True)
        if txt.startswith('ç›®æ¨™') or txt.startswith('éå»'):
            m = re.search(r'NT\$ *([\d,]+)', txt)
            if m:
                key = 'TargetPrice' if txt.startswith('ç›®æ¨™') else 'PastPrice'
                result[key] = int(m.group(1).replace(',', ''))

    # 3.6 è´ŠåŠ©äººæ•¸
    b = soup.select_one('span.js-backers-count')
    if b:
        result['Backers'] = int(b.get_text(strip=True).replace(',', ''))

    # 3.7 å‰©é¤˜æ™‚é–“ï¼ˆå¤©æ•¸ï¼‰
    t_node = soup.select_one('h3.js-time-left.text-zec-green')
    if t_node:
        m = re.search(r'(\d+)', t_node.get_text())
        if m:
            result['TimeLeftDays'] = int(m.group(1))

    # 3.8 å°ˆæ¡ˆæœŸé–“èµ·è¨–
    dur_txt = soup.select_one('h3.inline-block.text-gray-500.text-xs')
    if dur_txt:
        regex = (
            r"(\d{4}/\d{2}/\d{2}\s\d{2}:\d{2})"
            r"(?:\sâ€“\s(\d{4}/\d{2}/\d{2}\s\d{2}:\d{2}))?"
        )
        mt = re.search(regex, dur_txt.get_text())
        if mt:
            result['Duration'] = {
                'begin': mt.group(1),
                'end': mt.group(2) or ''
            }

    return result


def save_projects(path, projects):
    """æŠŠ list of dict å¯«å› JSON æª”æ¡ˆ"""
    try:
        with open(path, 'w', encoding='utf-8') as f:
            json.dump(projects, f, ensure_ascii=False, indent=2)
        print(f"âœ… å·²æˆåŠŸå°‡è³‡æ–™å¯«å…¥ {path}")
    except Exception as e:
        print(f"âŒ å¯«å…¥ {path} å¤±æ•—ï¼š{e!r}")




def main():
    # 1. è®€å–
    projects = load_projects(DATA_PATH)

    # 2. æ”¶é›†æ‰€æœ‰ link
    link_items = collect_link_items(projects)

    # 3. é€ç­†è™•ç†
    for i, (orig_idx, url) in enumerate(link_items, start=1):
        print(f"\n[{i}/{len(link_items)}] è™•ç†ï¼š{url}")
        if i == 5:  # for debugï¼Œåªè·‘å‰ 5 ç­†
            print("ğŸ”§ åœåœ¨ç¬¬ 5 ç­†åšæ¸¬è©¦")
            break

        # 3.1 å»º driver
        try:
            driver = init_driver()
            print("  âœ“ å•Ÿå‹• driver")
        except Exception as e:
            print(f"  X driver å•Ÿå‹•å¤±æ•—ï¼š{e!r}")
            time.sleep(SLEEP_ON_ERROR)
            continue

        # 3.2 è¼‰å…¥ä¸¦çˆ¬è³‡æ–™
        try:
            driver.get(url)
            result = scrape_project(driver, url, i)
        except Exception as e:
            print(f"  X çˆ¬å–éç¨‹å‡ºéŒ¯ï¼š{e!r}")
            driver.quit()
            print("  driver å·²é—œé–‰")
            time.sleep(SLEEP_ON_ERROR)
            continue
        finally:
            driver.quit()
            print("  driver.quit() å®Œæˆ")

        # 3.3 æ›´æ–°å› projects
        for k, v in result.items():
            projects[orig_idx][k] = v
            #result.items() æœƒå›å‚³ä¸€å€‹ç”± (key, value) tuple çµ„æˆçš„è¿­ä»£å™¨ 
            #ä¾‹å¦‚ï¼šresult = {'TargetPrice': 30000, 'Backers': 123} 
            #é‚£éº¼ result.items() æœƒç”¢ç”Ÿ ('TargetPrice', 30000)ã€('Backers', 123)ã€‚

        # 3.4 é¡¯ç¤ºçµæœ
        print("    â†³", result)

        # 3.5 ç­‰å¾…
        print(f"  â± ç­‰å¾… {SLEEP_BETWEEN} ç§’å¾Œç¹¼çºŒâ€¦")
        time.sleep(SLEEP_BETWEEN)

    # 4. å¯«å›
    save_projects(DATA_PATH, projects)

if __name__ == '__main__':
    main()
